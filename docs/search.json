[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Nima (NEE-MA). What you see here is created by Quarto! I did my PhD in analyzing renewable energy data. I also contribute to open-source projects. I am currently working as Machine Learning Engineer in the financial corporation Manulife."
  },
  {
    "objectID": "posts/confusion-matrix/index.html",
    "href": "posts/confusion-matrix/index.html",
    "title": "Confusion Matrix and Conditional Probability",
    "section": "",
    "text": "Let’s start with a quick overview of the confusion matrix. The confusion matrix is a table that is often used to describe the performance of a classification model. It summarizes the results of predictions made by the model, comparing them to the actual outcomes. Let’s consider a simple example of a binary classification problem, where we have two classes: positive (1) and negative (0). The confusion matrix for this problem may look like this:\n\n\n\n\n\n\n\n\n\nPredicted Positive (1)\nPredicted Negative (0)\n\n\n\n\nActual Positive (1)\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative (0)\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nTo measure the performance of the model, we can calculate several metrics based on the confusion matrix:\n\nAccuracy: The proportion of correct predictions (both true positives and true negatives) out of the total predictions.\n\\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)\nPrecision: The proportion of true positive predictions out of all positive predictions made by the model.\n\\(Precision = \\frac{TP}{TP + FP} = P(\\text{Actually Positive | Predicted as Positive})\\)\nFor instance, if the precision of a model is 0.8, it means that 80% of the instances predicted as positive are actually positive.\nRecall (Sensitivity): The proportion of true positive predictions out of all actual positive instances.\n\\(Recall = \\frac{TP}{TP + FN} = P(\\text{Predicted as Positive | Actually Positive})\\)\nFor instance, if the recall of a model is 0.9, it means that 90% of the actual positive instances are correctly predicted as positive. I may see it as “coverage” of the model, in a sense that it can correctly predict 90% of the actual positive instances.\nF1 Score: The harmonic mean of precision and recall, which balances the two metrics.\n\\(\\frac{1}{F1} = \\frac{(1/Precision) + (1/Recall)}{2}\\)\nSpecificity: The proportion of true negative predictions out of all actual negative instances.\n\\(Specificity = \\frac{TN}{TN + FP} = P(\\text{Predicted as Negative | Actually Negative})\\)\nFor instance, if the specificity of a model is 0.85, it means that 85% of the actual negative instances are correctly predicted as negative."
  },
  {
    "objectID": "posts/why-quarto/index.html",
    "href": "posts/why-quarto/index.html",
    "title": "Why Quarto?",
    "section": "",
    "text": "Why did I choose quarto for building my blog?\nI was looking for a simple way to publish posts. I tried to learn a little bit of HTML and CSS. But, I realized that organizing those files is not an easy task for me. And, in fact, I did not want to be worried about that! All I want was a simple tool that allows me to write some text and maybe provide some images, and then the tool can take care of the rest by rendering it to HTML format. There exists different static site generator, like HUGO or ASTRO. And maybe there are better choice. I decided to go with quarto because I was a bit familiar with it already and its guide for setting up blog was very straightforward."
  },
  {
    "objectID": "posts/args-and-kwargs/index.html",
    "href": "posts/args-and-kwargs/index.html",
    "title": "args and kwargs in Python",
    "section": "",
    "text": "What are *args and **kwargs in Python?\nI guess you have seen this syntax in Python functions, but you may not know what it means. Let’s taka a look at it!\n\ndef my_function(name, *args, **kwargs):\n    print(\"args:\", args)\n    print(\"kwargs:\", kwargs)    \n\n    return\n\nmy_function('nima', 1, 2, 3, a=4, b=5)\n\nargs: (1, 2, 3)\nkwargs: {'a': 4, 'b': 5}\n\n\nAs you can see, args is a variable of type tuple, and kwargs is a variable of type dict. Note that args can take any number of positional arguments, and kwargs can take any number of keyword arguments.\nTo read more, see: https://third-bit.com/sdxpy/oop/#oop-args"
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html",
    "href": "posts/CFA-Level-1-quant/index.html",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "",
    "text": "What is the value of an asset? It basically reflects the worth of a stream of future cash flows. To determine the value of an asset, we need to understand the “time value of money”. The “time value of money” deals with equivalence relationships between cash flows with different dates."
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html#introduction",
    "href": "posts/CFA-Level-1-quant/index.html#introduction",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "",
    "text": "What is the value of an asset? It basically reflects the worth of a stream of future cash flows. To determine the value of an asset, we need to understand the “time value of money”. The “time value of money” deals with equivalence relationships between cash flows with different dates."
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html#interest-rates",
    "href": "posts/CFA-Level-1-quant/index.html#interest-rates",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "Interest Rates",
    "text": "Interest Rates\nLet’s start with a quick example: Do you agree to pay $1000 today, and, in return, receive $950 today? Of course you don’t! You are basically losing $50 here! But… what if you need to pay it one year from now? This might be okay. A $1000 in one year from now should be worth less than $1000 today. So, it sounds to be fair to discount the (future) $1000 to calculate the amount that is received today.\nThe interest rate, \\(r\\), is a rate of return that reflects the relationship between differently dated cash flows. In our example above, $500 is the compensation required for paying $10000 one year from now.\n\\(r = \\frac{500}{9500} = 0.0526\\)\nSo, in this example, the rate of return is 5.26%.\nWe can look at the interest rate from three different perspectives. 1. It can be considered as “required” rate of return, which is the minimum interest rate that an investor should receive to accept an investment. 2. It can be considered as “discount rate”. In the example above, 5.26% is the discount rate that discounts the future cash flow of $1000 to determine the present value of $950. 3. It can be considered as “opportunity cost”. For instace, if we decide to not invest $950, and spend it instead, we are foregoing the opportunity to earn 5.26% on that amount."
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html#components-of-interest-rates",
    "href": "posts/CFA-Level-1-quant/index.html#components-of-interest-rates",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "Components of Interest Rates",
    "text": "Components of Interest Rates\nInterest rates are composed of several components, which can be summarized as follows:\n\\({\n  r = r_{real} + r_{inflation} + r_{risk} + r_{liquidity} + r_{maturity}\n}\\)\n\n\\(r_{real}\\): is “real risk-free interest rate”. This is a single-period interest rate that is expected when there is no other risks or inflation.\n\\(r_{inflation}\\): is the inflation premium. It compensates investors for the expected inflation rate.\n\\(r_{risk}\\): is the default risk permium. It compensates investors for the risk of default. This is the risk that the borrower will not be able to repay the loan.\n\\(r_{liquidity}\\): is the liquidity premium. Itcompensates investors for the risk of not being able to sell the asset quickly at a fair price.\n\\(r_{maturity}\\): is the maturity premium. It compensates investors for the increased sensitivity of the market value of debt to a change in market interest rates as maturity is extended.\n\nNote that \\(r_{real} + r_{inflation}\\) is known as the nominal risk-free interest rate."
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html#present-value-and-future-value",
    "href": "posts/CFA-Level-1-quant/index.html#present-value-and-future-value",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "Present Value and Future Value",
    "text": "Present Value and Future Value\nIn this section, we will try to understand the relationship between Present Value (PV) and Future Value (FV)\n\nFV of a single cash flow\nFor the given inital investment, \\(PV\\), and a given interest rate, \\(r\\), the future value of the investment after one period is:  \\(FV = PV \\times (1 + r)\\)  and after \\(n\\) periods is:  \\(FV_{n} = PV \\times (1+r)^{n}\\)\n\n\nNon-annual compounding\nSome banks may offer an interest that is compounded more frequently than annually. For instance, they might offer a monthly compounding interest rate. Financial institutions usually quote the annual interest rate, \\(r_{s}\\), which is known as the stated interest rate (a.k.a. quoted interest rate). For instance, a bank may state that a particular CD pays 7 percent compounded monthly. This means that the stated interest rate is 7 percent. The monthly interest rate is then calculated by dividing the stated interest rate by the number of compounding periods per year, \\(m\\). So, in this case, the monthly interest rate is: \\[r_{m} = \\frac{r_{s}}{m} = \\frac{0.07}{12} = 0.0058333\\].\nThe formula for future value is as follows:  \\(FV_{n} = PV \\times (1 + \\frac{r_{s}}{m})^{nm}\\)\nwhere \\(m\\) is the number of compounding periods per year, and \\(n\\) is the number of years.\nLet’s consider an example: Suppose you invest $1000 in a CD that pays 7 percent interest compounded monthly for 5 years. The future value of the investment at the end of 5 years is:\n\\(FV_{5} = 1000 \\times (1 + \\frac{0.07}{12})^{5 \\times 12}\\)  \\(FV_{5} = 1000 \\times (1 + 0.0058333)^{60}\\)  \\(FV_{5} = 1000 \\times (1.0058333)^{60}\\)  \\(FV_{5} = 1000 \\times 1.48985\\)  \\(FV_{5} = 1489.85\\) \nNOTE: In a special case where the interest rate is compounded continuously, the future value can be calculated using the following formula:  \\(FV_{n} = PV \\times e^{r_{s} \\cdot n}\\)\n\n\nEffective Annual Rate (EAR)\nThe Effective Annual Rate (EAR) is the annual interest rate that is equivalent to a given stated interest rate, \\(r_{s}\\), compounded more frequently than annually. So, basically we are trying to find \\(EAR\\) that satisfies the following equation:  \\(PV \\times (1 + r_{s} / m)^{m} = PV \\times (1 + EAR)\\)\nThis can be simplified to:  \\(EAR = (1 + r_{s} / m)^{m} - 1\\)\nAnd if the stated interest rate is compounded continuously, the EAR can be calculated as:  \\(EAR = e^{r_{s}} - 1\\)\n\n\nFuture Value of a Series of Cash Flows\nWhen we have a series of cash flows, we can calculate the future value of the series by summing the future values of each individual cash flow. TBC"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my blog. Welcome!\n\nMy goal is to encourage myself to write about cool stuff. This can help me with gaining new skills as well as improving my writing skills. I think my future self will thank me! I set up blog twice before but didn’t like them. I like this one though! Let’s see if I can stick to this one!!"
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html",
    "href": "posts/LLM-gentle-intro/index.html",
    "title": "A gentle intro to LLM",
    "section": "",
    "text": "The information provided below is based on the Sebastian Raschka book “Build LLM from Scratch”."
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#intro",
    "href": "posts/LLM-gentle-intro/index.html#intro",
    "title": "A gentle intro to LLM",
    "section": "Intro",
    "text": "Intro\nLLM stands for Large Language Model. This model is a (large) deep neural network trained on a large amount of data. At its core, LLM uses “transformer”, an architecture that allows it to pay attention to different parts of input selectively.\nWhat can LLM do for us? * Text generation: LLM can generate text based on a given prompt, which can be used for creative writing, content generation, and more. * Text generation with control: LLM can generate text with specific attributes, such as style, tone, and sentiment, which can be used for personalized content generation, marketing, and more. * Text completion: LLM can complete a given text based on the context, which can be used for auto-completion, code generation, and more. * Text classification: LLM can classify text into different categories, which can be used for sentiment analysis, topic classification, and more. * Text summarization: LLM can summarize a given text, which can be used for news summarization, document summarization, and more. * Text translation: LLM can translate text from one language to another, which can be used for machine translation, localization, and more. * Text question answering: LLM can answer questions based on a given text, which can be used for information retrieval, chatbots, and more. * Text sentiment analysis: LLM can analyze the sentiment of a given text, which can be used for social media analysis, customer feedback analysis, and more.\nSo, LLM can be useful when we deal with parsing and generating text."
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#stages-of-building-llm",
    "href": "posts/LLM-gentle-intro/index.html#stages-of-building-llm",
    "title": "A gentle intro to LLM",
    "section": "Stages of building LLM",
    "text": "Stages of building LLM\nThere are several ready-to-use LLMs available, such as OpenAI’s GPT-3 or Google’s BERT. However, you may want to build your own custom-built LLM for specific tasks. One example is BloombergGPT, which is a custom-built LLM based on financial data. Why should you want to build your own LLM? * Customization: You can tailor the model to your specific needs and requirements. * Control: You have full control over the model architecture, training data, and hyperparameters. * Privacy: You can keep your data private and secure, without relying on third-party providers. * Cost: You can save costs by using your own hardware and resources, rather than paying for cloud-based services.\n* Performance: You can optimize the model for your specific use case, which can lead to better performance and accuracy.\nThe general process of building LLM consists of two stages:  1. Pre-training: This stage involves training the model on a large corpus of text data to learn the underlying patterns and relationships in the language. The model learns to predict the next word in a sentence given the previous words, which helps it understand the structure and semantics of the language.  2. Fine-tuning: This stage involves training the pre-trained model (a.k.a foundation model) on a smaller, task-specific dataset to adapt it to a specific task or domain. Fine-tuning allows the model to learn the nuances and specifics of the task, improving its performance and accuracy.\n\n\n\n\n\nflowchart LR\n    A[Raw unlabeled text data] --&gt; B[[pre-training]]\n    B --&gt; C[foundation model]\n    C --&gt; E[[fine-tuning]]\n    D[labled data] --&gt; E\n    E --&gt; F[fine-tuned LLM]\n\n\n\n\n\n\n\nRaw unlabled text data: This is the data that is used to pre-train the model. It can be any text data, such as books, articles, or web pages. For instance, let’s look at this simple sentence: “The cat sat on the mat.” The model will learn to predict the next word in the sentence given the previous words, such as “The cat sat on the” -&gt; “mat”. So, in the pre-training stage, the model learns the structure and semantics of the language by predicting the next word in a sentence given the previous words.\nfoundation model: This is the pre-trained model that has learned the underlying patterns and relationships in the language. It can do “text completion” and has “few-shot” capabilities, meaning it can be adapted to specific tasks with a small amount of labeled data.\nlabled data: This is the data that is used to fine-tune the model. It can be any text data that is labeled for a specific task, such as sentiment analysis, text classification, or question answering. For instance, if we want to fine-tune the model for sentiment analysis, we can use a dataset of movie reviews labeled as positive or negative.\nfine-tuned LLM: This is the final model that has been fine-tuned on the labeled data for a specific task. It can be used for various applications, such as sentiment analysis, text classification, or question answering.\n\nNote on fine-tuning:  There are two popular approaches for fine-tuning LLMS: (i) instruction fine-tuning, and (ii) classification fine-tuning. In the “instruction fine-tuning” approach, data consists of pairs of (instruction, answer). In the “classification fine-tuning” approach, however, the data contains pairs of (text, label)."
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#transformer",
    "href": "posts/LLM-gentle-intro/index.html#transformer",
    "title": "A gentle intro to LLM",
    "section": "transformer",
    "text": "transformer\nIn a very high-level view, one can see transformer as follows:\n\n\n\n\n\nflowchart LR\n    A[Input text] --&gt; B[Encoder]\n    B --&gt; C[encoded vectors]\n    C --&gt; D[Decoder]\n    D --&gt; E[Output text]\n\n\n\n\n\n\nA key component is self-attention mechanism (not shown in the diagram above). This allows the model to consider different weights for different words (or tokens) in the input text, depending on their importance in the context of the sentence. Both GPT and BERT use transformer architecture. While GPT is good at text generation (text completion), BERT is good at masked word prediction. So BERT may show better performance in tasks such as text classification, while GPT may show better performance in tasks such as text generation.\nFurther reading:  * See the Appendix B of the book “Build LLM from Scratch” by Sebastian Raschka. * Read the original paper “Attention is all you need” by Vaswani et al. (2017)"
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#large-data-set-for-pre-training",
    "href": "posts/LLM-gentle-intro/index.html#large-data-set-for-pre-training",
    "title": "A gentle intro to LLM",
    "section": "large data set for pre-training",
    "text": "large data set for pre-training\nMany LLMs are pre-trained on very large datasets, and the associated cost of such pre-training can be very high. For instance, OpenAI’s GPT-3 was trained on 570GB of text data, which cost around $4.6 million to pre-train. However, you can use smaller datasets for pre-training, such as the Common Crawl dataset, which is a publicly available dataset of web pages. Also, there are many open-source pre-trained models (foundation models) available. For the sake of education, we will try to pre-train small LLM on a small dataset. Once done, we will then switch to use an open-souce pre-trained model (foundation model) and fine-tune it on a small dataset."
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#gpt-architecture",
    "href": "posts/LLM-gentle-intro/index.html#gpt-architecture",
    "title": "A gentle intro to LLM",
    "section": "GPT architecture",
    "text": "GPT architecture\nTBC"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataExplorer",
    "section": "",
    "text": "Confusion Matrix and Conditional Probability\n\n\n\nMachine Learning\n\nProbability\n\n\n\nUnderstanding the confusion matrix and its relation to conditional probability.\n\n\n\n\n\nJun 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInterest Rates, Present Value, and Future Value\n\n\n\nCFA\n\nQuant\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nargs and kwargs in Python\n\n\n\npython\n\n\n\nUnderstanding *args and **kwargs in Python functions\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Quarto?\n\n\n\nquarto\n\nblog\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA gentle intro to LLM\n\n\n\nLLM\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 24, 2025\n\n\n\n\n\nNo matching items"
  }
]