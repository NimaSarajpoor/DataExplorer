[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Nima (NEE-MA). What you see here is created by Quarto! I did my PhD in analyzing renewable energy data. I also contribute to open-source projects. I am currently working as Machine Learning Engineer in the financial corporation Manulife."
  },
  {
    "objectID": "posts/confusion-matrix/index.html",
    "href": "posts/confusion-matrix/index.html",
    "title": "Confusion Matrix and Conditional Probability",
    "section": "",
    "text": "Let’s start with a quick overview of the confusion matrix. The confusion matrix is a table that is often used to describe the performance of a classification model. It summarizes the results of predictions made by the model, comparing them to the actual outcomes. Let’s consider a simple example of a binary classification problem, where we have two classes: positive (1) and negative (0). The confusion matrix for this problem may look like this:\n\n\n\n\n\n\n\n\n\nPredicted Positive (1)\nPredicted Negative (0)\n\n\n\n\nActual Positive (1)\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative (0)\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nTo measure the performance of the model, we can calculate several metrics based on the confusion matrix:\n\nAccuracy: The proportion of correct predictions (both true positives and true negatives) out of the total predictions.\n\\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)\nPrecision: The proportion of true positive predictions out of all positive predictions made by the model.\n\\(Precision = \\frac{TP}{TP + FP} = P(\\text{Actually Positive | Predicted as Positive})\\)\nFor instance, if the precision of a model is 0.8, it means that 80% of the instances predicted as positive are actually positive.\nRecall (Sensitivity): The proportion of true positive predictions out of all actual positive instances.\n\\(Recall = \\frac{TP}{TP + FN} = P(\\text{Predicted as Positive | Actually Positive})\\)\nFor instance, if the recall of a model is 0.9, it means that 90% of the actual positive instances are correctly predicted as positive. I may see it as “coverage” of the model, in a sense that it can correctly predict 90% of the actual positive instances.\nF1 Score: The harmonic mean of precision and recall, which balances the two metrics.\n\\(\\frac{1}{F1} = \\frac{(1/Precision) + (1/Recall)}{2}\\)\nSpecificity: The proportion of true negative predictions out of all actual negative instances.\n\\(Specificity = \\frac{TN}{TN + FP} = P(\\text{Predicted as Negative | Actually Negative})\\)\nFor instance, if the specificity of a model is 0.85, it means that 85% of the actual negative instances are correctly predicted as negative."
  },
  {
    "objectID": "posts/daily-grand/index.html",
    "href": "posts/daily-grand/index.html",
    "title": "$1000 for life or lump sum of $7 million?",
    "section": "",
    "text": "Let’s suppose you win the daily grand lottery, which gives you $1000 every day for the rest of your life, or a lump sum of $7 million. Which option should you choose? To make the informed decision, we need to evaluate the “VALUE” of each option. The second one is straightforward, as it is a lump sum of $7 million. The first one, however, requires us to calculate the present value of the annuity (the daily payments). Suppose you are 35 years old when you win the lottery, and let’s say you will live until 85 years old. This means you will receive $1000 every day for 50 years. Let’s say your bank offers interest rate of 3% per year, and for the sake of simplicity, we will assume that the interest is compounded annually, and it is fixed for the next 50 years. Let’s assume we deposit money at the end of each year, meaning: \\(A=1000 \\times 365 = 365000\\) \n\\(PV = A \\times \\frac{(1+r)^n - 1}{r} \\times \\frac{1}{(1+r)^n}\\)  \\(PV = 365000 \\times \\frac{(1+0.03)^{50} - 1}{0.03} \\times \\frac{1}{(1+0.03)^{50}}\\)  \\(PV = 365000 \\times \\frac{(1.03)^{50} - 1}{0.03} \\times \\frac{1}{(1.03)^{50}}\\)  \\(PV = 365000 \\times \\frac{4.384 - 1}{0.03} \\times \\frac{1}{4.384}\\)  \\(PV = 365000 \\times \\frac{3.384}{0.03} \\times \\frac{1}{4.384}\\)  \\(PV = 365000 \\times 112.8 \\times 0.228\\)  \\(PV \\approx 9387216\\) \nSo, this shows that the present value of the annuity is approximately $9.4 million, which is greater than the lump sum of $7 million. Therefore, you should choose the daily payments of $1000 for life.\nNOTE: This was a simplified calculation. In reality, you may need to think about different things like \n\nDo I live till 85 years old? If not, then is it better to take the lump sum?\nWhat if bank changes the interest rate?\n\n I’ve not considered these factors here, but they are important to consider in real life. How? do not know! But I think it is important to think about them!!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my blog. Welcome!\n\nMy goal is to encourage myself to write about cool stuff. This can help me with gaining new skills as well as improving my writing skills. I think my future self will thank me! I set up blog twice before but didn’t like them. I like this one though! Let’s see if I can stick to this one!!"
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html",
    "href": "posts/CFA-Level-1-quant/index.html",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "",
    "text": "What is the value of an asset? It basically reflects the worth of a stream of future cash flows. To determine the value of an asset, we need to understand the “time value of money”. The “time value of money” deals with equivalence relationships between cash flows with different dates."
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html#introduction",
    "href": "posts/CFA-Level-1-quant/index.html#introduction",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "",
    "text": "What is the value of an asset? It basically reflects the worth of a stream of future cash flows. To determine the value of an asset, we need to understand the “time value of money”. The “time value of money” deals with equivalence relationships between cash flows with different dates."
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html#interest-rates",
    "href": "posts/CFA-Level-1-quant/index.html#interest-rates",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "Interest Rates",
    "text": "Interest Rates\nLet’s start with a quick example: Do you agree to pay $1000 today, and, in return, receive $950 today? Of course you don’t! You are basically losing $50 here! But… what if you need to pay it one year from now? This might be okay. A $1000 in one year from now should be worth less than $1000 today. So, it sounds to be fair to discount the (future) $1000 to calculate the amount that is received today.\nThe interest rate, \\(r\\), is a rate of return that reflects the relationship between differently dated cash flows. In our example above, $500 is the compensation required for paying $10000 one year from now.\n\\(r = \\frac{500}{9500} = 0.0526\\)\nSo, in this example, the rate of return is 5.26%.\nWe can look at the interest rate from three different perspectives. 1. It can be considered as “required” rate of return, which is the minimum interest rate that an investor should receive to accept an investment. 2. It can be considered as “discount rate”. In the example above, 5.26% is the discount rate that discounts the future cash flow of $1000 to determine the present value of $950. 3. It can be considered as “opportunity cost”. For instace, if we decide to not invest $950, and spend it instead, we are foregoing the opportunity to earn 5.26% on that amount."
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html#components-of-interest-rates",
    "href": "posts/CFA-Level-1-quant/index.html#components-of-interest-rates",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "Components of Interest Rates",
    "text": "Components of Interest Rates\nInterest rates are composed of several components, which can be summarized as follows:\n\\({\n  r = r_{real} + r_{inflation} + r_{risk} + r_{liquidity} + r_{maturity}\n}\\)\n\n\\(r_{real}\\): is “real risk-free interest rate”. This is a single-period interest rate that is expected when there is no other risks or inflation.\n\\(r_{inflation}\\): is the inflation premium. It compensates investors for the expected inflation rate.\n\\(r_{risk}\\): is the default risk permium. It compensates investors for the risk of default. This is the risk that the borrower will not be able to repay the loan.\n\\(r_{liquidity}\\): is the liquidity premium. Itcompensates investors for the risk of not being able to sell the asset quickly at a fair price.\n\\(r_{maturity}\\): is the maturity premium. It compensates investors for the increased sensitivity of the market value of debt to a change in market interest rates as maturity is extended.\n\nNote that \\(r_{real} + r_{inflation}\\) is known as the nominal risk-free interest rate."
  },
  {
    "objectID": "posts/CFA-Level-1-quant/index.html#present-value-and-future-value",
    "href": "posts/CFA-Level-1-quant/index.html#present-value-and-future-value",
    "title": "Interest Rates, Present Value, and Future Value",
    "section": "Present Value and Future Value",
    "text": "Present Value and Future Value\nIn this section, we will try to understand the relationship between Present Value (PV) and Future Value (FV)\n\nFV of a single cash flow\nFor the given inital investment, \\(PV\\), and a given interest rate, \\(r\\), the future value of the investment after one period is:  \\(FV = PV \\times (1 + r)\\)  and after \\(n\\) periods is:  \\(FV_{n} = PV \\times (1+r)^{n}\\)\n\n\nNon-annual compounding\nSome banks may offer an interest that is compounded more frequently than annually. For instance, they might offer a monthly compounding interest rate. Financial institutions usually quote the annual interest rate, \\(r_{s}\\), which is known as the stated interest rate (a.k.a. quoted interest rate). For instance, a bank may state that a particular CD pays 7 percent compounded monthly. This means that the stated interest rate is 7 percent. The monthly interest rate is then calculated by dividing the stated interest rate by the number of compounding periods per year, \\(m\\). So, in this case, the monthly interest rate is: \\[r_{m} = \\frac{r_{s}}{m} = \\frac{0.07}{12} = 0.0058333\\].\nThe formula for future value is as follows:  \\(FV_{n} = PV \\times (1 + \\frac{r_{s}}{m})^{nm}\\)\nwhere \\(m\\) is the number of compounding periods per year, and \\(n\\) is the number of years.\nLet’s consider an example: Suppose you invest $1000 in a CD that pays 7 percent interest compounded monthly for 5 years. The future value of the investment at the end of 5 years is:\n\\(FV_{5} = 1000 \\times (1 + \\frac{0.07}{12})^{5 \\times 12}\\)  \\(FV_{5} = 1000 \\times (1 + 0.0058333)^{60}\\)  \\(FV_{5} = 1000 \\times (1.0058333)^{60}\\)  \\(FV_{5} = 1000 \\times 1.48985\\)  \\(FV_{5} = 1489.85\\) \nNOTE: In a special case where the interest rate is compounded continuously, the future value can be calculated using the following formula:  \\(FV_{n} = PV \\times e^{r_{s} \\cdot n}\\)\n\n\nEffective Annual Rate (EAR)\nThe Effective Annual Rate (EAR) is the annual interest rate that is equivalent to a given stated interest rate, \\(r_{s}\\), compounded more frequently than annually. So, basically we are trying to find \\(EAR\\) that satisfies the following equation:  \\(PV \\times (1 + r_{s} / m)^{m} = PV \\times (1 + EAR)\\)\nThis can be simplified to:  \\(EAR = (1 + r_{s} / m)^{m} - 1\\)\nAnd if the stated interest rate is compounded continuously, the EAR can be calculated as:  \\(EAR = e^{r_{s}} - 1\\)\n\n\nFuture Value of a Series of Cash Flows\nWhen we have a series of cash flows, we can calculate the future value of the series by summing the future values of each individual cash flow. Let’s start with defining some terminolgies:\n\nAnnuity: A series of equal cash flows received at regular intervals. For instance, if you receive $1000 every year for 5 years, this is an annuity.\n\nOrdinary Annuity: An annuity where the cash flows are received at the end of each period. For instance, if you receive $1000 at the end of each year for 5 years, this is an ordinary annuity.\nAnnuity Due: An annuity where the cash flows are received at the beginning of each period. For instance, if you receive $1000 at the beginning of each year for 5 years, this is an annuity due.\n\nPerpetuity: A series of cash flows that continues indefinitely. For instance, if you receive $1000 every year forever, this is a perpetuity.\n\nNOTE: Another way to understand the difference between Ordinary Annuity and Annuity Due is that Ordinary Annuity is a series of cash flows that are received starting at t=1, and Annuity Due is a series of cash flows that are received starting at t=0.\nNow that we know the definition, we can compute their value.\n\nOrdinary Annuity with payment $A for n periods at interest rate r\ntimeline: t=0, t=1 ($A), t=2 ($A), …, t=n ($A)\n\nCalculate the future value of this asset at the end:  \\(FV_{n} = A(1 + r)^{n-1} + A(1 + r)^{n-2} + ... + A(1 + r) + A\\)  \\(FV_{n} = A \\left( (1 + r)^{n-1} + (1 + r)^{n-2} + ... + (1 + r) + 1 \\right)\\)  \\(FV_{n} = A \\times \\frac{(1 + r)^{n} - 1}{r}\\)\nCalculate the present value of this asset (at t=0):  \\(PV = \\frac{A}{1 + r} + \\frac{A}{(1+r)^2} + ... + \\frac{A}{(1+r)^{n}}\\)  \\(PV = A \\left( \\frac{1}{1 + r} + \\frac{1}{(1+r)^2} + ... + \\frac{1}{(1+r)^{n}} \\right)\\) \nwith \\(d = \\frac{1}{1 + r}\\), we will have:  \\(PV = A \\left( d + d^2 + ... + d^{n} \\right)\\)  \\(PV = A \\cdot d \\left(1 + d + ... + d^{n-1}\\right)\\)  \\(PV = A \\cdot d \\cdot \\frac{d^{n} - 1}{d - 1}\\) \nAlternaively, we could have just used the future value \\(FV_{n} = A \\times \\frac{(1 + r)^{n} - 1}{r}\\) to compute the present value as follows:  \\(PV = \\frac{FV_{n}}{(1 + r)^{n}}\\)\n\n\n\nAnnuity Due with payment $A for n periods at interest rate r\ntimeline: t=0 ($A), t=1 ($A), t=2 ($A), …, t=n-1 ($A)\nLet’s Calculate the present value of this asset at t=0: \n\\(PV = A + A(1+r)^{-1} + A(1+r)^{-2} + ... + A(1+r)^{n-1}\\)  \\(PV = A \\left(1 + (1+r)^{-1} + ... + (1+r)^{n-1}\\right)\\)  \\(PV = A \\left(1 + d + ... + d^{n-1}\\right)\\), where \\(d = (1+r)^{-1}\\)  \\(PV = A \\frac{(1 + d)^{n} - 1}{d - 1}\\), where \\(d = (1+r)^{-1}\\) \n\n\nPerpetuity with payment $A at interest rate r\ntimeline: t=0, t=1 ($A), t=2 ($A), …\n\\(PV = A(1+r)^{-1} + ...\\)  \\(PV = Ad(1 + d + ...)\\), where \\(d = \\frac{1}{1+r} &lt; 1\\)  \\(PV = \\frac{Ad}{1 - d}\\), where \\(d = \\frac{1}{1+r}\\)  \\(PV = \\frac{A}{r}\\)\nNOTE: We can use the simple formula of Perpetuity to compute PV for Annuity. Let’s see how that works. Suppose we have an ordinary annuity with payment $A for n periods at interest rate r. So, the timeline looks like this: timeline: t=0, t=1 ($A), t=2 ($A), …, t=n-1 ($A)\nThis can be considered as the substraction of two perpetuities: 1. A perpetuity with timeline t=0, t=1 (A), t=2 (A), … 2. A perpetuity with timeline t=0, t=1, t=2, …, t=n (A), t=n+1 (A), …\n\\(PV = PV1 - PV2\\), where the PVs are at time t=0  \\(PV = \\frac{A}{r} - \\frac{A}{r} \\frac{1}{(1+r)^{n-1}}\\) \nRegarding PV2, note that \\(\\frac{A}{r}\\) gives the present value of the second perpetuity at time t=n-1. So, it needs to be discounted to time t=0 by dividing it by \\((1+r)^{n-1}\\)."
  },
  {
    "objectID": "posts/args-and-kwargs/index.html",
    "href": "posts/args-and-kwargs/index.html",
    "title": "args and kwargs in Python",
    "section": "",
    "text": "What are *args and **kwargs in Python?\nI guess you have seen this syntax in Python functions, but you may not know what it means. Let’s taka a look at it!\n\ndef my_function(name, *args, **kwargs):\n    print(\"args:\", args)\n    print(\"kwargs:\", kwargs)    \n\n    return\n\nmy_function('nima', 1, 2, 3, a=4, b=5)\n\nargs: (1, 2, 3)\nkwargs: {'a': 4, 'b': 5}\n\n\nAs you can see, args is a variable of type tuple, and kwargs is a variable of type dict. Note that args can take any number of positional arguments, and kwargs can take any number of keyword arguments.\nTo read more, see: https://third-bit.com/sdxpy/oop/#oop-args"
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html",
    "href": "posts/LLM-gentle-intro/index.html",
    "title": "A gentle intro to LLM",
    "section": "",
    "text": "The information provided below is based on the Sebastian Raschka book “Build LLM from Scratch”."
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#intro",
    "href": "posts/LLM-gentle-intro/index.html#intro",
    "title": "A gentle intro to LLM",
    "section": "Intro",
    "text": "Intro\nLLM stands for Large Language Model. This model is a (large) deep neural network trained on a large amount of data. At its core, LLM uses “transformer”, an architecture that allows it to pay attention to different parts of input selectively.\nWhat can LLM do for us? * Text generation: LLM can generate text based on a given prompt, which can be used for creative writing, content generation, and more. * Text generation with control: LLM can generate text with specific attributes, such as style, tone, and sentiment, which can be used for personalized content generation, marketing, and more. * Text completion: LLM can complete a given text based on the context, which can be used for auto-completion, code generation, and more. * Text classification: LLM can classify text into different categories, which can be used for sentiment analysis, topic classification, and more. * Text summarization: LLM can summarize a given text, which can be used for news summarization, document summarization, and more. * Text translation: LLM can translate text from one language to another, which can be used for machine translation, localization, and more. * Text question answering: LLM can answer questions based on a given text, which can be used for information retrieval, chatbots, and more. * Text sentiment analysis: LLM can analyze the sentiment of a given text, which can be used for social media analysis, customer feedback analysis, and more.\nSo, LLM can be useful when we deal with parsing and generating text."
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#stages-of-building-llm",
    "href": "posts/LLM-gentle-intro/index.html#stages-of-building-llm",
    "title": "A gentle intro to LLM",
    "section": "Stages of building LLM",
    "text": "Stages of building LLM\nThere are several ready-to-use LLMs available, such as OpenAI’s GPT-3 or Google’s BERT. However, you may want to build your own custom-built LLM for specific tasks. One example is BloombergGPT, which is a custom-built LLM based on financial data. Why should you want to build your own LLM? \n\nCustomization: You can tailor the model to your specific needs and requirements.\nControl: You have full control over the model architecture, training data, and hyperparameters.\nPrivacy: You can keep your data private and secure, without relying on third-party providers.\nCost: You can save costs by using your own hardware and resources, rather than paying for cloud-based services.\n\nPerformance: You can optimize the model for your specific use case, which can lead to better performance and accuracy.\n\nThe general process of building LLM consists of two stages:  1. Pre-training: This stage involves training the model on a large corpus of text data to learn the underlying patterns and relationships in the language. The model learns to predict the next word in a sentence given the previous words, which helps it understand the structure and semantics of the language.  2. Fine-tuning: This stage involves training the pre-trained model (a.k.a foundation model) on a smaller, task-specific dataset to adapt it to a specific task or domain. Fine-tuning allows the model to learn the nuances and specifics of the task, improving its performance and accuracy.\n\n\n\n\n\nflowchart LR\n    A[Raw unlabeled text data] --&gt; B[[pre-training]]\n    B --&gt; C[foundation model]\n    C --&gt; E[[fine-tuning]]\n    D[labled data] --&gt; E\n    E --&gt; F[fine-tuned LLM]\n\n\n\n\n\n\n\nRaw unlabled text data: This is the data that is used to pre-train the model. It can be any text data, such as books, articles, or web pages. For instance, let’s look at this simple sentence: “The cat sat on the mat.” The model will learn to predict the next word in the sentence given the previous words, such as “The cat sat on the” -&gt; “mat”. So, in the pre-training stage, the model learns the structure and semantics of the language by predicting the next word in a sentence given the previous words.\nfoundation model: This is the pre-trained model that has learned the underlying patterns and relationships in the language. It can do “text completion” and has “few-shot” capabilities, meaning it can be adapted to specific tasks with a small amount of labeled data.\nlabled data: This is the data that is used to fine-tune the model. It can be any text data that is labeled for a specific task, such as sentiment analysis, text classification, or question answering. For instance, if we want to fine-tune the model for sentiment analysis, we can use a dataset of movie reviews labeled as positive or negative.\nfine-tuned LLM: This is the final model that has been fine-tuned on the labeled data for a specific task. It can be used for various applications, such as sentiment analysis, text classification, or question answering.\n\nNote on fine-tuning:  There are two popular approaches for fine-tuning LLMS: (i) instruction fine-tuning, and (ii) classification fine-tuning. In the “instruction fine-tuning” approach, data consists of pairs of (instruction, answer). In the “classification fine-tuning” approach, however, the data contains pairs of (text, label)."
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#transformer",
    "href": "posts/LLM-gentle-intro/index.html#transformer",
    "title": "A gentle intro to LLM",
    "section": "transformer",
    "text": "transformer\nIn a very high-level view, one can see transformer as follows:\n\n\n\n\n\nflowchart LR\n    A[Input text] --&gt; B[Encoder]\n    B --&gt; C[encoded vectors]\n    C --&gt; D[Decoder]\n    D --&gt; E[Output text]\n\n\n\n\n\n\nA key component is self-attention mechanism (not shown in the diagram above). This allows the model to consider different weights for different words (or tokens) in the input text, depending on their importance in the context of the sentence. Both GPT and BERT use transformer architecture. While GPT is good at text generation (text completion), BERT is good at masked word prediction. So BERT may show better performance in tasks such as text classification, while GPT may show better performance in tasks such as text generation.\nFurther reading:  * See the Appendix B of the book “Build LLM from Scratch” by Sebastian Raschka. * Read the original paper “Attention is all you need” by Vaswani et al. (2017)"
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#large-data-set-for-pre-training",
    "href": "posts/LLM-gentle-intro/index.html#large-data-set-for-pre-training",
    "title": "A gentle intro to LLM",
    "section": "large data set for pre-training",
    "text": "large data set for pre-training\nMany LLMs are pre-trained on very large datasets, and the associated cost of such pre-training can be very high. For instance, OpenAI’s GPT-3 was trained on 570GB of text data, which cost around $4.6 million to pre-train. However, you can use smaller datasets for pre-training, such as the Common Crawl dataset, which is a publicly available dataset of web pages. Also, there are many open-source pre-trained models (foundation models) available. For the sake of education, we will try to pre-train small LLM on a small dataset. Once done, we will then switch to use an open-souce pre-trained model (foundation model) and fine-tune it on a small dataset."
  },
  {
    "objectID": "posts/LLM-gentle-intro/index.html#gpt-architecture",
    "href": "posts/LLM-gentle-intro/index.html#gpt-architecture",
    "title": "A gentle intro to LLM",
    "section": "GPT architecture",
    "text": "GPT architecture\nTBC"
  },
  {
    "objectID": "posts/system-design/index.html",
    "href": "posts/system-design/index.html",
    "title": "system design: where to start?",
    "section": "",
    "text": "According to wikipedia, the basic study of system design is the understanding of component parts and their subsequent interaction with one another. Systems design has appeared in a variety of fields including computer/software architecture."
  },
  {
    "objectID": "posts/system-design/index.html#what-is-system-design",
    "href": "posts/system-design/index.html#what-is-system-design",
    "title": "system design: where to start?",
    "section": "",
    "text": "According to wikipedia, the basic study of system design is the understanding of component parts and their subsequent interaction with one another. Systems design has appeared in a variety of fields including computer/software architecture."
  },
  {
    "objectID": "posts/system-design/index.html#when-did-i-first-face-challenges-in-system-design",
    "href": "posts/system-design/index.html#when-did-i-first-face-challenges-in-system-design",
    "title": "system design: where to start?",
    "section": "When did I first face challenges in system design?",
    "text": "When did I first face challenges in system design?\nI was working on a project that was running 24/7. I was taksed with finding a solution for backfilling a large amount of data that was not processed in the past since the system was down. I had to think about different aspects such as:  * How to ensure that the system can handle the large amount of data without crashing?  * How to ensure that the data is processed in a timely manner?  * What to do if the system fails during the backfilling process?  * Can it run in parallel with the daily processing? Is there any dependency?  * What was the reason of failure? Has there been any change in the format of incoming data? \nEach system design problem is unique. However, I think there should be some ways to improve such skills. In programming, we know that each problem can be unique. But there are some practices like leetcode that can help us to improve our skills. I think the same applies to system design. So, I did a quick search and I am going to share what I’ve found online."
  },
  {
    "objectID": "posts/system-design/index.html#resources-to-learn-system-design",
    "href": "posts/system-design/index.html#resources-to-learn-system-design",
    "title": "system design: where to start?",
    "section": "Resources to learn system design",
    "text": "Resources to learn system design\n\nSystem Design Interview – An insider’s guide\nDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems\nGrokking the System Design Interview\ncodemia system design\nsystem design primer\ncodewars\nblogs to follow"
  },
  {
    "objectID": "posts/system-design/index.html#first-step",
    "href": "posts/system-design/index.html#first-step",
    "title": "system design: where to start?",
    "section": "First step?",
    "text": "First step?\nI am not sure but as I was reading opinions on reddit, I realized that I should probably start with the book System Design Interview – An insider’s guide."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "$1000 for life or lump sum of $7 million?\n\n\n\nCFA\n\nQuant\n\n\n\nUnderstanding the time value of money\n\n\n\n\n\nJun 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nsystem design: where to start?\n\n\n\nsystem design\n\n\n\nFinding a roadmap for system design can be challenging!\n\n\n\n\n\nJun 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix and Conditional Probability\n\n\n\nMachine Learning\n\nProbability\n\n\n\nUnderstanding the confusion matrix and its relation to conditional probability.\n\n\n\n\n\nJun 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInterest Rates, Present Value, and Future Value\n\n\n\nCFA\n\nQuant\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nargs and kwargs in Python\n\n\n\npython\n\n\n\nUnderstanding *args and **kwargs in Python functions\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA gentle intro to LLM\n\n\n\nLLM\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 24, 2025\n\n\n\n\n\nNo matching items"
  }
]